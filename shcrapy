#!/bin/sh
# shcrapy - a tool to crawl and collect links from a website
#
# Copyright (c) 2024-2025, Victor Tschetter
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
#
# * Redistributions of source code must retain the above copyright notice, this
#   list of conditions and the following disclaimer.
#
# * Redistributions in binary form must reproduce the above copyright notice,
#   this list of conditions and the following disclaimer in the documentation
#   and/or other materials provided with the distribution.
#
# * Neither the name of the copyright holder nor the names of its
#   contributors may be used to endorse or promote products derived from
#   this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

usage() {
	echo "Usage: shcrapy [-h|-f|-e [extension(s)]|-i [word(s)] <URL>"
	echo "	-e | --extensions [extension(s)]  comma separated list of extensions to scrape for"
	echo "	-i | --ignore     [word(s)]       comma separated list of words to ignore in URLS"
	echo "	-f | --force                      crawl all links, including extensions and special characters (?,#)"
	echo "	-h | --help                       show this help message"
	echo "Example: shcrapy -f -e mp3,zip,pdf -i keyword1,keyword2 domain.example.com"
	exit 1
}

if ! command -v lynx > /dev/null; then
	echo "Error: Lynx text browser is not installed."
	echo "Please install Lynx to proceed."
	exit 1
fi

EXTENSIONS=""
IGNORE=""
FORCE=0
while [ "$#" -gt 0 ]; do
    case "${1}" in
	-h|--help|help)
	    usage
	    ;;
        -e|--extensions)
            EXTENSIONS="${2}"
            shift 2
            ;;
        -i|--ignore)
            IGNORE="${2}"
            shift 2
            ;;
        -f|--force)
            FORCE=1
            shift
            ;;
        -*)
            echo "Unknown option: \"${1}\""
            exit 1
            ;;
        *)
            break
            ;;
    esac
done

if [ "$#" -ne 1 ]; then
	usage
fi

BASE_URL="${1}"
BASE_DOMAIN="$(echo $BASE_URL | sed "s|http.*://||" | awk -F'/' '{print $1}')"

if ! ping -c 1 "${BASE_DOMAIN}" > /dev/null; then
	echo "${BASE_DOMAIN} is down, or incorrectly formatted"
	exit 1
fi

if [ -z ${EXTENSIONS} ]; then
	echo "Error: [-e|--extensions] must be set."
	usage
fi

EXTENSIONS=$(echo "${EXTENSIONS}" | tr ',' '|')
IGNORE=$(echo "${IGNORE}" | tr ',' '|')

OUTPUT_FILE="$(echo "${BASE_DOMAIN}" | tr '\/' '_')"_downloadable_files.txt
if [ ! -f "${OUTPUT_FILE}" ]; then
    touch "${OUTPUT_FILE}"
fi

VISITED_FILE="$(echo "${BASE_DOMAIN}" | tr '\/' '_')"_visited_urls.txt
# Always reset visited file
: > "$VISITED_FILE"

crawl() {
	url="$1"

    	# Get links from the current URL and extract full URLs
    	if ! grep -Fxq "${url}" "${VISITED_FILE}"; then
        	echo "${url}" >> "${VISITED_FILE}"
        	lynx -dump -nonumbers "${url}" | grep -Eo "http[^ ]+\.(${EXTENSIONS})$" | while read -r file_url; do
            		if ! grep -Fxq "${file_url}" "${OUTPUT_FILE}"; then
                		echo "${file_url}" >> "${OUTPUT_FILE}"
                		echo "added ${file_url} to download list"
            		fi
        	done
    	fi

	# Get all links to follow
	if echo "${url}" | grep -q "${BASE_URL}"; then
		lynx -dump -nonumbers "${url}" | grep -E "${BASE_DOMAIN}/[^ ]+" | sort -u | while read -r link; do
			if ! grep -Fxq "${link}" "${VISITED_FILE}"; then
				# Check the extensions and ignore conditions
				if echo "${link}" | grep -qoE "[?#<>]+" && [ "${FORCE}" -ne 1 ]; then
					continue
				fi
                		if echo "${link}" | grep -Eoq "\.[a-zA-Z0-9]+$" && ! echo "${link}" | grep -Eoq "\.(php|html|htm)$" && [ "${FORCE}" -ne 1 ]; then
                			continue
                		elif [ -n "${IGNORE}" ] && echo "${link}" | grep -qoE "(${IGNORE})"; then
                			continue
                		else
                			crawl "${link}"
                		fi
    			fi
        	done
	fi
}

# Start crawling from the base URL
crawl "${BASE_URL}"

# Count the number of collected links and print the result
link_count=$(wc -l < "${OUTPUT_FILE}")
echo "Finished. Collected ${link_count} links."
